<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Hector' log</title><link href="/hectormorago/" rel="alternate"></link><link href="/hectormorago/feeds/all.atom.xml" rel="self"></link><id>/hectormorago/</id><updated>2020-07-07T20:31:00+02:00</updated><subtitle>A personal blog.</subtitle><entry><title>Normalización</title><link href="/hectormorago/normalizacion.html" rel="alternate"></link><published>2020-07-07T20:31:00+02:00</published><updated>2020-07-07T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-07-07:/hectormorago/normalizacion.html</id><summary type="html">&lt;p&gt;Teoria de normalizacion de BBDD.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"&gt;
&lt;style type="text/css"&gt;
table {
  border-spacing: 0;
  border-collapse: collapse;
}
td,
th {
  padding: 0;
}
.thead-light th{color:#495057;background-color:#e9ecef;border-color:#dee2e6}
.table-dark{color:#fff;background-color:#212529}
&lt;/style&gt;
&lt;script src="https://polyfill.io/v3/polyfill.min.js?features=es6"&gt;&lt;/script&gt;
&lt;script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt; La &lt;strong&gt;normalización&lt;/strong&gt; se ocupa de comprobar mediante un método sistematico si una determinada relación es correcta, es decir, que no presenta redundancias. En el caso de que una relación, R, presente redundancias, deberemos de proceder a su descomposición de modo que las relaciones resultantes {\( R_{1}, R_{2}, R_{3},  ..., R_{n} \)} sean &lt;strong&gt;correctas&lt;/strong&gt; y &lt;strong&gt;no&lt;/strong&gt; presenten &lt;strong&gt;inconsistencias&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Al realizar el diseño de una base de datos (BBDD), ya sea obteniendo el diseño relacional directamente o mediante el proceso en dos pasos (diseño conceptual y transformación a relacional), pueden aparecer los siguientes &lt;strong&gt;problemas&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redundancias.&lt;/li&gt;
&lt;li&gt;Ambigüedades.&lt;/li&gt;
&lt;li&gt;Pérdida de información.&lt;/li&gt;
&lt;li&gt;Existencia de valores no aplicables o desconocidos.&lt;/li&gt;
&lt;li&gt;Aparición de estados que no son válidos en el mundo real.&lt;/li&gt;
&lt;li&gt;Pérdidas de relaciones entre los datos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con el fin de evitar estos problemas surge la teoría de la normalización. A continuación tenemos una relación de ejemplo una relación que presenta algunos de los problemas anteriormente citados.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estudiante_solicita_beca&lt;/strong&gt;&lt;/p&gt;
&lt;table class="table"&gt;
  &lt;thead class="thead-light"&gt;
    &lt;tr&gt;
      &lt;th scope="col"&gt;#&lt;/th&gt;
      &lt;th scope="col"&gt;cod_est&lt;/th&gt;
      &lt;th scope="col"&gt;nombre_est&lt;/th&gt;
      &lt;th scope="col"&gt;cod_beca&lt;/th&gt;
      &lt;th scope="col"&gt;nombre_beca&lt;/th&gt;
      &lt;th scope="col"&gt;fecha&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th scope="row"&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/th&gt;
      &lt;td&gt;Roberto&lt;/td&gt;
      &lt;td&gt;B1&lt;/td&gt;
      &lt;td&gt;Erasmus&lt;/td&gt;
      &lt;td&gt;2020-01-21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th scope="row"&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/th&gt;
      &lt;td&gt;Gema&lt;/td&gt;
      &lt;td&gt;B2&lt;/td&gt;
      &lt;td&gt;Apoyo biblioteca&lt;/td&gt;
      &lt;td&gt;2018-02-22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th scope="row"&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/th&gt;
      &lt;td&gt;Laura&lt;/td&gt;
      &lt;td&gt;B2&lt;/td&gt;
      &lt;td&gt;Apoyo biblioteca&lt;/td&gt;
      &lt;td&gt;2018-02-22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th scope="row"&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/th&gt;
      &lt;td&gt;Miguel&lt;/td&gt;
      &lt;td&gt;B3&lt;/td&gt;
      &lt;td&gt;Investigación&lt;/td&gt;
      &lt;td&gt;2020-03-24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th scope="row"&gt;5&lt;/th&gt;
      &lt;td&gt;2&lt;/th&gt;
      &lt;td&gt;Laura&lt;/td&gt;
      &lt;td&gt;B1&lt;/td&gt;
      &lt;td&gt;Erasmus&lt;/td&gt;
      &lt;td&gt;2020-01-21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th scope="row"&gt;6&lt;/th&gt;
      &lt;td&gt;3&lt;/th&gt;
      &lt;td&gt;Miguel&lt;/td&gt;
      &lt;td&gt;B1&lt;/td&gt;
      &lt;td&gt;Erasmus&lt;/td&gt;
      &lt;td&gt;2020-01-21&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;De un simple vistazo podemos ver algunos de los problemas mencionados. Por ejemplo, estamos almacenando de forma redundante los datos de un estudiante cada vez que este realiza una solicitud (con los datos de las becas pasa los mismo). Otros problemas que podemos observar son: inconsistencias debidas a la actualización de los datos de un estudiante o una beca en sólo algunas de las tuplas, problemas para almacenar los datos de estudiantes que no han solicitado becas o de becas que no han sido solicitadas, problemas en el borrado de solicitudes (perderíamos los datos del estudiante en el caso de sólo hubiese solicitado esa beca), etc.&lt;/p&gt;
&lt;p&gt;Con el fin de paliar estos problemas surge la teoría de la Normalización. Podemos afirmar que la teoría de la normalización permite afrontar el problema de diseño de BBDD relacionales de una manera rigurosa y objetiva.&lt;/p&gt;
&lt;p&gt;Un aspecto crucial en la normalización es el concepto de &lt;strong&gt;Dependencia Funcional&lt;/strong&gt;. Las dependencias nos permiten representar interdependencias entre los datos. Son propiedades inherentes al contenido semántico de los datos y forman parte de las restricciones de usuario del Modelo Relacional. Se han de cumplir en cualquier extensión de un esquema de relación, es decir, son invariantes en el tiempo. Aquí cabe destacar que no es posible deducir una dependencia a partir de la observación de una extensión del esquema de relación.&lt;/p&gt;
&lt;p&gt;Existen diferentes tipos de dependencias: funcionales, multivaluadas, jerárquicas y de combinación. Sin embargo, en esta ocasión vamos a hablar sólo de las funcionales.&lt;/p&gt;
&lt;p&gt;Para el estudio de las dependencias vamos a considerar que el esquema relacional está compuesto por una única relación, \(R(AT, DEP)\), donde \(AT\) es el conjunto de atributos de la relación \(R\) y \(DEP\) es el conjunto de dependencias existentes entre los atributos.&lt;/p&gt;

&lt;p&gt;Una dependencia va tener el siguiente aspecto:  \(A \rightarrow B\), donde \(A, B \subseteq AT\). La parte de la izquierda recibe el nombre de determinante o implicante, mientras que la parte de la derecha recibe el nombre implicado.&lt;/p&gt;

&lt;p&gt;Por ejemplo: \(Cod\_Estudiante \rightarrow Nombre\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definición&lt;/strong&gt;: Sea el esquema de relación \(R(A,DF)\), y sean \(X, Y \subseteq A\), a los que llamamos descriptores. Se dice que \(Y\) depende funcionalmente de \(X\) o que \(X\) implica o determina a \(Y\), y se denota como \(X \rightarrow Y\) si, y sólo si, \( \forall x \in X, \exists y \in Y \cdot x \rightarrow y \wedge \nexists y' \cdot x \rightarrow y' \).&lt;/p&gt;

&lt;p&gt;Otras consecuencia importante que introduce el concepto de dependencia es que restrigen el conjunto de valores que un atributo determinado puede tomar. Continuando con el ejemplo anterior, si \(Cod\_Estudiante\) toma el valor 2, \(nombre\) sólo puede tomar el valor Laura. Por otro lado, si no existiese esa dependencia, \(nombre\) podría tomar cualquier valor del dominio sobre el que esta definido. Además, y adelandonos al concepto de clave que veremos más adelante, podemos afirmar que si \(A \subseteq AT\), es clave, \(A \rightarrow AT - A\), es decir, el resto de atributos de la relación dependenden de él. En caso contrario, es decir, A no es clave (o clave candidata), entonces se introduce cierta redundancia.&lt;/p&gt;

&lt;p&gt;Este último aspecto, junto al hecho de que a mayor número de dependencias mayor sobrecarga se introduce en el Sistema de Gestión de Base Datos (SGBD), hace que sea importante tener en cuenta sólo aquellas dependencias (entre los datos que estamos modelando) que sean estrictamente necesarias.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propiedades&lt;/strong&gt; de la dependencias funcionales:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Dos &lt;strong&gt;descriptores&lt;/strong&gt; X e Y son &lt;strong&gt;equivalentes&lt;/strong&gt; si: &lt;p&gt;\(X \rightarrow Y\) e \(Y \rightarrow X\), o bien, \(X \leftrightarrow Y\).&lt;/p&gt;
Ejemplo: \(Cod\_Estudiante \leftrightarrow DNI\)
&lt;li&gt; Una &lt;strong&gt;dependencia&lt;/strong&gt; se dice que es &lt;strong&gt;trivial&lt;/strong&gt; si el implicado esta incluido en el implicante. Si \(X \rightarrow Y\), \(Y \subseteq X\), entonces \(X \rightarrow Y\) es trivial.

Ejemplo: \(Cod\_curso, Cod\_edicion \rightarrow Cod\_curso\)

&lt;li&gt; &lt;strong&gt;Dependencia funcional plena&lt;/strong&gt;, sea la dependencia \(X \rightarrow Y\) se dice que es plena, si \(\nexists X', X' \subset X \cdot X' \rightarrow Y\). En caso contrario se dice que no es plena, y que los atributos \(X-X'\), son extraños. 

Ejemplo: \(Cod\_curso, Cod\_edicion \rightarrow Programa\) (suponemos que todos los cursos tienen el mismo programa independientemente de su edición). En este caso diríamos que \(Cod\_edicion\) es un &lt;strong&gt;atributo extraño&lt;/strong&gt;.

&lt;li&gt; &lt;strong&gt;Dependencia funcional transitiva&lt;/strong&gt;. Sea \(R(\{X,Y,Z\}, \{X \rightarrow Y, Y \rightarrow Z\})\), decimos que \(Z\) depende tansitivamente de \(X\). Si además \(Z \nrightarrow Y\) se dice que es estricta. 

Ejemplo: \(R(\{CP, Localidad, Provincia\}, \{CP \rightarrow Localidad, Localidad \rightarrow Provincia\})\), decimos que \(Provincia\) depende transitivamente de \(CP\). Además, como \(Provincia \nrightarrow Localidad\), entonces decimos que es estricta.
&lt;/ul&gt;

&lt;p&gt;Otro aspecto importante en relación a las dependencias funcionales es el concepto de &lt;strong&gt;cierre de un conjunto de dependencias funcionales&lt;/strong&gt;. El cierre de un conjunto de dependencias funcionales, \(DF^+\), es el conjunto de todas las dependencias que pertenecen a \(DF\) o pueden derivarse de este (por ejemplo por la aplicación de los &lt;strong&gt;axiomas de armstrong&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Axiomas de armstrong&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Reflesividad: si \(Y \subset X \wedge X \rightarrow Y, X \rightarrow Y \) es trivial.
&lt;li&gt; Aumento: si \(X \rightarrow Y, XZ \rightarrow YZ\) 
&lt;li&gt; Transitividad: si \(X \rightarrow Y \wedge Y \rightarrow Z, X \rightarrow Z\) 
&lt;/ul&gt;

&lt;p&gt;Se puede demostrar que toda dependencia \(X \rightarrow Y\) derivada de \(DF\) mediante la aplicación de los axiomas está en \(DF^+\).&lt;/p&gt;

&lt;p&gt;Por ejemplo, si queremos saber si una DF es cierta, es decir, si \(DF \in DF^+\): Calcular el cierre de un subconjunto, tal que \(X\) sea implicante: cierre transitivo de un descriptor \(X\) de \(R\) respecto al conjunto de dependencias \(DF\), \(X_{DF}^+\)&lt;/p&gt;

&lt;p&gt;Dado \(R(AT, DF)\), se define \(X_{DF}^+\) como un subconjunto de los atributos de \(AT\) tales que \(X \rightarrow X_{DF}^+ \in DF^+\), siendo \(X_{DF}^+\) máximo en el sentido de que la adición de cualquier atributo vulneraría la condición anterior.&lt;/p&gt;

&lt;p&gt;Consecuencias: con \(X_{DF}^+\)  es posible el cálculo de superclaves (SK) y claves (K)&lt;/p&gt;

&lt;p&gt;Esta no es la única forma de determinar si una \(DF\) es cierta. Por ejemplo, podemos utilizar herramientas como los SAT Solver o las librerías de BDD's para responder a esta pregunta. Un SAT Solver es una herramienta que nos permite determinar si una fórmula lógica en Forma Normal Conjuntiva (CNF) es satisfacible (existe al menos una asignación en la cual la fórmula se evalua a cierto) o no. Por otro lado, un BDD (binary decision diagram), es un grafo dirigido aciclico que nos permite representar de forma compacta la tabla de verdad de una forma lógica. En cualquiera de los dos casos, el proceso requiere de algunos conocimientos de lógica proposicional.&lt;/p&gt;

&lt;p&gt;Una forma de calcular el cierre de un descriptor es mediante el &lt;strong&gt;Algoritmo de Ullman&lt;/strong&gt; que utiliza el concepto de transitividad es las dependencias. Vamos a ver con un ejemplo cual es su funcionamiento.&lt;/p&gt;

&lt;p&gt; Sea R ({A, B, C, D, E, F}, DF), donde DF = { &lt;/p&gt;

&lt;p&gt;
    \(A \rightarrow B, B \rightarrow A, C \rightarrow A, D \rightarrow C, (E, C) \rightarrow D, A \rightarrow F, C \rightarrow F\) 
&lt;/p&gt;

&lt;p&gt; } &lt;/p&gt;

&lt;p&gt;Hallar el cierre del descriptor (E, C)&lt;/p&gt;

&lt;p&gt; \((E, C)^+\) = {E, C}  &lt;/p&gt;

&lt;p&gt; \((E, C)^+\) = {E, C, D} &lt;/p&gt;

&lt;p&gt; \((E, C)^+\) = {E, C, D, A} &lt;/p&gt;

&lt;p&gt; \((E, C)^+\) = {E, C, D, A, B} &lt;/p&gt;

&lt;p&gt; \((E, C)^+\) = {E, C, D, A, B, F} &lt;/p&gt;

&lt;p&gt;Por tanto, \((E, C)^+\) = {E, C, D, A, B, F} por lo que (E,C) es superclave.&lt;/p&gt;

&lt;p&gt;
Nos interesa saber si una dependencia es cierta. Por ejemplo, ¿\(B \rightarrow F\)? 
\(A \rightarrow B\) y \(B \rightarrow A\) (son equivalentes) por lo tanto \(B \rightarrow F\)
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bibliografia&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tecnología y Diseño de Bases de Datos. M. Piattini, E. Marcos, C. Calero y B. Vela Ed.: RAMA. 2006&lt;/li&gt;
&lt;li&gt;Date, C. J. "An Introduction to Database Systems" (8ª edición), Addison-Wesley, 2004. (Versión en español: "Introducción a los Sistemas de Bases de Datos" (7ª edición), Pearson, 2001&lt;/li&gt;
&lt;li&gt;Batini, C., Ceri, S., Navathe, S. "Diseño Conceptual de Bases de Datos. Un enfoque de entidades-interrelaciones." Addison-Wesley Iberoamericana, 1994.&lt;/li&gt;
&lt;li&gt;Elsmari, R. y Navathe, S. B. "Sistemas de Bases de Datos. Conceptos Fundamentales." Addison-Wesley Iberoamericana, 1997. &lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category><category term="BBDD"></category><category term="teoria"></category><category term="otros"></category></entry><entry><title>About Me</title><link href="/hectormorago/about-me.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/about-me.html</id><summary type="html">&lt;p&gt;This section provides a summarize of my research interests.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"&gt;
&lt;style&gt; 
    .label {
    display: inline;
    padding: .2em .6em .3em;
    font-size: 75%;
    font-weight: 700;
    line-height: 1;
    color: #fff;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
    border-radius: .25em;
}
.label-success{background-color:#5cb85c}
.label-success[href]:focus,.label-success[href]:hover{background-color:#449d44}
.label-danger{background-color:#d9534f}
.label-danger[href]:focus,.label-danger[href]:hover{background-color:#c9302c}
.label-warning{background-color:#f0ad4e}
.label-warning[href]:focus,.label-warning[href]:hover{background-color:#ec971f}
&lt;/style&gt;&lt;/p&gt;
&lt;h2&gt;Research interest&lt;/h2&gt;
&lt;p&gt;My research interests include, but are not limited to Variability Modeling, 
Software Product Lines, Feature Models, software testing, Binary Decision 
Diagrams, and Bibliometric Techniques.&lt;/p&gt;
&lt;h2&gt;Publications&lt;/h2&gt;
&lt;h3&gt;Speeding up Derivative Configuration from Product Platforms&lt;/h3&gt;
&lt;p&gt;Heradio, R.; Fernandez-Amoros, D.; &lt;strong&gt;Perez-Morago, H.&lt;/strong&gt;; Adan, A&lt;/p&gt;
&lt;div&gt;
    &lt;img src="https://www.mdpi.com/img/journals/entropy-logo.png?78b1902e596e9c35" alt="entropy-logo" title="entropy Open Access Journals" style="float: left; max-height: 50px; margin: 0 0 0 0; "&gt;
    &lt;p&gt;
        &lt;strong&gt;Abstract&lt;/strong&gt; : To compete in the global marketplace, manufacturers try to differentiate their products by focusing on individual customer needs. Fulfilling this goal requires that companies shift from mass production to mass customization. Under this approach, a generic architecture, named product platform, is designed to support the derivation of customized products through a configuration process that determines which components the product comprises. When a customer configures a derivative, typically not every combination of available components is valid. To guarantee that all dependencies and incompatibilities among the derivative constituent components are satisfied, automated configurators are used. Flexible product platforms provide a big number of interrelated components, and so, the configuration of all, but trivial, derivatives involves considerable effort to select which components the derivative should include. Our approach alleviates that effort by speeding up the derivative configuration using a heuristic based on the information theory concept of entropy. View Full-Text
    &lt;/p&gt;
    &lt;div id='journal'&gt;
        &lt;spam class="label label-success"&gt;Journal Paper&lt;/spam&gt;
        Entropy June 2014, Volume 17, Pages 3329-3356
    &lt;/div&gt;
    &lt;div id='impact-factor'&gt;
        &lt;spam class="label label-danger"&gt;Impact Factor&lt;/spam&gt;
        &lt;strong id='impact-factor-number-entropy'&gt;&lt;/strong&gt;
        (Journal Citation Reports) | 2nd Quartile
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;Augmenting Measure Sensitivity to Detect Essential, Dispensable and Highly Incompatible Features in Mass Customization&lt;/h3&gt;
&lt;p&gt;Heradio, R.; &lt;strong&gt;Perez-Morago, H.&lt;/strong&gt;; Alferez, M.; Fernandez-Amoros, D.; Alferez G.&lt;/p&gt;
&lt;div&gt;
    &lt;img src="https://ars.els-cdn.com/content/image/X03772217.jpg" alt="ejor-logo" title="European Journal of Operational Research" style="float: left; max-height: 100px; margin: 0 0 0 0;"&gt;
    &lt;p&gt;
        &lt;strong&gt;Abstract&lt;/strong&gt; : Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis.Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.
    &lt;/p&gt;
    &lt;div id='journal'&gt;
        &lt;spam class="label label-success"&gt;Journal Paper&lt;/spam&gt;
        European Journal of Operational Research, August 2015, 248(3):1066-1077
    &lt;/div&gt;
    &lt;div id='impact-factor'&gt;
        &lt;spam class="label label-danger"&gt;Impact Factor&lt;/spam&gt;
        &lt;strong id='impact-factor-number-ejor'&gt;2.679&lt;/strong&gt;
        (Journal Citation Reports) | 1st Quartile
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;A Bibliometric Analysis of 20 Years of Research on Software Product Lines&lt;/h3&gt;
&lt;p&gt;Heradio, R.; &lt;strong&gt;Perez-Morago, H.&lt;/strong&gt;; Fernandez-Amoros, D.; Cabrerizo, 
Francisco Javier; Herrera-Viedma, Enrique&lt;/p&gt;
&lt;div&gt;
    &lt;img src="https://ars.els-cdn.com/content/image/X09505849.jpg" alt="ist-logo" title="Information and Software Technology" style="float: left; max-height: 100px; margin: 0 0 0 0; padding:2px;"&gt;
    &lt;div id='journal'&gt;
        &lt;p&gt;
        &lt;strong&gt;Abstract&lt;/strong&gt; : Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis.Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.
        &lt;/p&gt;
        &lt;spam class="label label-success"&gt;Journal Paper&lt;/spam&gt;
        Information and Software Technology, 72, April 2016
    &lt;/div&gt;
    &lt;div id='impact-factor'&gt;
        &lt;spam class="label label-danger"&gt;Impact Factor&lt;/spam&gt;
        &lt;strong id='impact-factor-number-ejor'&gt;1.569&lt;/strong&gt;
        (Journal Citation Reports) | 2nd Quartile
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;Efficient Identification of Core and Dead Features in Variability Models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Hector Perez-Morago&lt;/strong&gt;, Ruben Heradio, David Fernandez-Amoros, Roberto Bean and Jose Antonio Cerrada &lt;/p&gt;
&lt;div&gt;
    &lt;img src="https://ieeeaccess.ieee.org/wp-content/uploads/2016/05/IEEE_ACCESS_Logo-noTagline_FullColor_RGB-plus-margin-7800x2000-X-1000.png" alt="ieee_access-logo" title="IEEE Access" style="float: left ;max-height: 50px; margin: 0 0 0 0; "&gt;
    &lt;p&gt;
        &lt;strong&gt;Abstract&lt;/strong&gt; : Mass customization enables the creation of personalized products that fulfill the features desired by specific customers. In this context, variability models are used to specify which configurable features are supported and which constraints among the features must be satisfied to guarantee the validity of the derived products. As the market demand grows and evolves, variability models become increasingly complex. In such entangled models, it is hard to identify which features are absolutely essential or dispensable because they are required to be included or excluded from all the possible products, respectively. This paper exposes the limitations of existing approaches to automatically detect those features and provides an algorithm that efficiently performs such detection.
    &lt;/p&gt;
    &lt;div id='journal'&gt;
        &lt;spam class="label label-success"&gt;Journal Paper&lt;/spam&gt;
        IEEE Access. Nov 2015.
    &lt;/div&gt;
    &lt;div id='impact-factor'&gt;
        &lt;spam class="label label-danger"&gt;Impact Factor&lt;/spam&gt;
        &lt;strong id='impact-factor-number-ejor'&gt;1.249&lt;/strong&gt;
        (Journal Citation Reports) | 1st Quartile
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;Science Mapping Analysis of the Literature on Software Product Lines.&lt;/h3&gt;
&lt;p&gt;Heradio, R.; &lt;strong&gt;Perez-Morago, H.&lt;/strong&gt;; Fernandez-Amoros, D.; Cabrerizo, Francisco Javier; 
Herrera-Viedma, Enrique&lt;/p&gt;
&lt;div&gt;
    &lt;img src="https://images.springer.com/sgw/books/medium/9783319226880.jpg" alt="somet2015-logo" title="Somet 2015" style="float: left ;max-height: 100px; margin: 0 0 0 0; padding:2px;"&gt;
    &lt;p&gt;
        &lt;strong&gt;Abstract&lt;/strong&gt; : To compete in the global marketplace, manufacturers try to differentiate their products by focusing on individual customer needs. Fulfilling this goal requires companies to shift from mass production to mass customization. In the context of software development, software product line engineering has emerged as a cost effective approach to developing families of similar products by support high levels of mass customization. This paper analyzes the literature on software product lines from its beginnings to 2014. A science mapping approach is applied to identify the most researched topics, and how the interest in those topics has evolved along the way.
    &lt;/p&gt;
    &lt;div id='journal'&gt;
        &lt;spam class="label label-warning"&gt;Conference Paper&lt;/spam&gt;
        The 14th International Conference on Intelligent Software Methodologies, Tools and Techniques Monumental Complex of St. Chiara, Naples, Italy September 15-17, 2015
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;Binary Decision Diagram Algorithms to Perform Hard Analysis Operations on Variability Models.&lt;/h3&gt;
&lt;p&gt;Heradio, R.; &lt;strong&gt;Perez-Morago, H.&lt;/strong&gt;; Fernandez-Amoros, D.; 
Bean, Roberto; Cabrerizo, Francisco Javier; Cerrada, Carlos; Herrera-Viedma, Enrique
Conference Paper The 15th International Conference on Intelligent Software 
Methodologies, Tools and Techniques. Larnaca, Cyprus, 12-14, 2016&lt;/p&gt;
&lt;div&gt;
    &lt;img src="https://www.cyprusconferences.org/somet2016/img/somet2016logonotitle.png" alt="somet2015-logo" title="Somet 2015" style="float: left ;max-height: 60px; margin: 0 0 0 0; padding:2px;"&gt;
    &lt;p&gt;
        &lt;strong&gt;Abstract&lt;/strong&gt; : Variability models play a key role in software product line engineering as they are used to represent the common and variable features that products may include, and what constraints among the features must be satisfied to guarantee the validity of the products. Valuable analysis operations on variability models can be performed by black box reusing logic engines, such as SAT-solvers and binary decision diagram libraries. Unfortunately, such kind of reuse implies long computation times for operations that need calling the engines many times. To overcome this problem, we propose new algorithms that directly deal with the data structure of a binary decision diagram encoding a variability model. In particular, our algorithms are specifically designed to detect core &amp; dead features, and the impact &amp; exclusion sets of every feature.
    &lt;/p&gt;
    &lt;div id='journal'&gt;
        &lt;spam class="label label-warning"&gt;Conference Paper&lt;/spam&gt;
        The 14th International Conference on Intelligent Software Methodologies, Tools and Techniques. Larnaca, Cyprus, 12-14, 2016
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Phd dissertation&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;BDD Algorithms to Perform Hard Analysis Operations on Variability Models.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Héctor José Pérez Morago&lt;/strong&gt; 
Universidad Nacional de Educación a Distancia (España). Escuela Internacional de Doctorado. Programa de Doctorado en Ingeniería de Sistemas y Control&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;
To compete in the global marketplace, manufacturers try to differentiate their products by focusing on individual customer needs. Fulfilling this goal requires companies to shift from mass production to mass customization. In the context of software development, software product line engineering has emerged as a cost-effective approach to developing families of similar products by supporting high levels of mass customization.
Variability models are often used to specify the common and variable features of the products in one such family. Moreover, they model the inter-feature constraints which must be satisfied to guarantee the validity of the derived products. Despite the benefits of variability models, constructing and maintaining them can be a laborious task, especially in product lines with a large number of features and constraints. As a consequence, the study of automated techniques to reason on variability models has become an important research topic for the product line community.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://e-spacio.uned.es/fez/view/tesisuned:IngInf-Hjperez"&gt;&lt;strong&gt;Full version&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;
  var cors_api_url = 'https://cors-anywhere.herokuapp.com/';
  function doCORSRequest(options, printResult) {
    var x = new XMLHttpRequest();
    x.open(options.method, cors_api_url + options.url);
    x.onload = x.onerror = function() {
      var parser = new DOMParser();
      var doc = parser.parseFromString(x.responseText, "text/html");
      elem = doc.getElementsByClassName("header-impact-factor-sci")[0];
      elem = elem.getElementsByClassName("number")[0].innerHTML;
      printResult((elem || ''));
    };
    x.send();
  }

  // Bind event
  (function() {
    var urlField = document.getElementById('url');
    var outputField = document.getElementById('impact-factor-number-entropy');
    window.onload = function(e) {
      e.preventDefault();
      doCORSRequest({
        method: 'GET',
        url: 'https://mdpi.com/journal/entropy/stats#if',
      }, function printResult(result) {
        //console.log(result);
        outputField.innerHTML += result;
      });
    };
  })();
&lt;/script&gt;</content><category term="About Me"></category><category term="research"></category><category term="publications"></category><category term="others"></category></entry><entry><title>Algoritmo de planificación First Come First Served</title><link href="/hectormorago/alg-fcfs.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/alg-fcfs.html</id><summary type="html">&lt;p&gt;Algoritmo de planificación First Come First Served.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;FCFS&lt;/h2&gt;
&lt;p&gt;Primero en llegar primero en ser servido, FCFS (First Come First Served). Es uno de los algoritmos de planificación 
más sencillos de implementar. El planificador maneja la cola de procesos como una cola FIFO. Entre sus &lt;strong&gt;ventajas&lt;/strong&gt; destacamos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facilidad de implementación.&lt;/li&gt;
&lt;li&gt;Poca sobrecarga.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sus principales &lt;strong&gt;desventajas&lt;/strong&gt; son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Penaliza los procesos con tiempos de ráfaga más cortos con respecto a aquellos que tienen tiempo de ráfaga más largos.&lt;/li&gt;
&lt;li&gt;Puede aparecer lo que se denomina como efecto convoy.&lt;/li&gt;
&lt;li&gt;Sufre la anomalía de belady.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category><category term="SSOO"></category><category term="algoritmos"></category><category term="otros"></category></entry><entry><title>Planificación de procesos</title><link href="/hectormorago/alg-planificacion.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/alg-planificacion.html</id><summary type="html">&lt;p&gt;Revisión de los principales algoritmos de planificación.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"&gt;
En &lt;strong&gt;sistemas operativos multitarea&lt;/strong&gt;, donde podemos ejecutar cientos procesos al mismo tiempo,
necesitamos de herramientas que de algún modo &lt;strong&gt;multiplexen los recursos hardware&lt;/strong&gt; del sistema. Una de estas herramientas 
la proporciona el sistema operativo (SO), mediante el &lt;strong&gt;planificador&lt;/strong&gt; o scheduler. 
El planificador se encarga de repartir la CPU entre los procesos que se encuentran en el sistema listos para ejecutarse. 
Su funcionamiento básico se basa en la forma en la que los procesos se ejecutan, que consiste en realizar de forma alternativa 
ráfagas de CPU y de E/S (ver Figura 1). En función de la duración de dichas ráfagas podemos distinguir entre procesos que se 
encuentran limitados por la CPU (donde la duración de las ráfagas de CPU es mucho mayor que las de E/S) y procesos que se 
encuentran limitados por la E/S (donde la duración de las ráfagas de E/S es mucho mayor que las de E/S). El SO consciente de 
este funcionamiento aprovecha las ráfagas de E/S para ejecutar otro proceso, ya que en caso contrario la CPU se quedaría inactiva. &lt;/p&gt;
&lt;p&gt;En general podemos distinguir tres &lt;strong&gt;niveles de planificación&lt;/strong&gt;: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a corto plazo&lt;/strong&gt;, se encarga de decidir el siguiente proceso que pasará a ejecutarse de entre  aquellos que se encuentran 
preparados para su ejecución. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;a medio plazo&lt;/strong&gt;, decide entre los procesos que están en memoria secundaria cual pasa a memoria principal, y viceversa. 
Es decir, regula el grado de multiprogramación. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;a largo plazo&lt;/strong&gt;, decide qué trabajo de la cola de trabajo por lotes pasa a ser ejecutado en el sistema mediante la creación 
de un proceso.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Aparte de estos tres niveles de planificación se puede distinguir un cuarto nivel, la &lt;strong&gt;planificación de E/S&lt;/strong&gt; que
decide qué proceso en una cola de un dispositivo de E/S pasa a utilizar dicho dispositivo.&lt;/p&gt;
&lt;p&gt;La parte del SO que se encarga de realizar las tareas de planificación puede dividirse en los siguientes componentes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encolador&lt;/strong&gt;  o enqueuer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conmutador de contexto&lt;/strong&gt; o context switcher.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribuidor&lt;/strong&gt;,  despachador o dispatcher. Se encarga de seleccionar un proceso de la cola de procesos preparados de 
acuerdo con un determinado algoritmo de planificación.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Es importante destacar que la tarea de planificación introduce &lt;strong&gt;sobrecarga&lt;/strong&gt;, es decir, tiempo que el SO está dedicado a tareas que no repercute en los procesos de los usuarios y que, por tanto, hay que minimizar. A la hora de seleccionar un algortimo de planificación podemos utilizar determinadas métricas como: &lt;strong&gt;Latencia de despacho&lt;/strong&gt;, es el tiempo promedio que tarda el distribuidor en detener un proceso y comenzar la ejecución de otro. Lo deseable que la latencia de despacho sea lo más pequeña posible. Sin embargo, muchos
SOs con objeto de asegurar la integridad de sus estructuras de datos no permiten que se pueda expropiar un proceso en modo núcleo que esté realizando una llamada al sistema, hasta que ésta no se complete o el proceso entre en el estado bloqueado. Tales SOs se dicen que son de núcleo no expropiable. En dicho caso la latencia de despacho puede ser larga ya que algunas llamadas al sistema son complejas.&lt;/p&gt;
&lt;p&gt;En general a la hora de planificar procesos vamos a tener en cuenta una serie de criterios generales:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Previsibilidad. Tareas similares igual tiempo.&lt;/li&gt;
&lt;li&gt;Uso equilibrado de recursos.&lt;/li&gt;
&lt;li&gt;Proporcionalidad. Tareas que parecen sencillas deben ejecutarse en plazos relativamente cortos.&lt;/li&gt;
&lt;li&gt;Equidad.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entre los &lt;strong&gt;principales algoritmos de planificación&lt;/strong&gt; podemos citar los siguientes: 
&lt;a href="https://hperez30.github.io/alg-fcfs/"&gt;FCFS&lt;/a&gt;, &lt;a href="https://hperez30.github.io/alg-sjf/"&gt;SJF&lt;/a&gt;, 
&lt;a href="https://hperez30.github.io/alg-srt/"&gt;SRT&lt;/a&gt;, &lt;a href="https://hperez30.github.io/alg-round-robin/"&gt;Round Robin&lt;/a&gt;, etc.&lt;/p&gt;</content><category term="blog"></category><category term="SSOO"></category><category term="algoritmos"></category><category term="otros"></category></entry><entry><title>Algoritmo de planificación Round Robin</title><link href="/hectormorago/alg-round-robin.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/alg-round-robin.html</id><summary type="html">&lt;p&gt;Algoritmo de planificación Round Robin.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Round Robin&lt;/h2&gt;</content><category term="blog"></category><category term="SSOO"></category><category term="algoritmos"></category><category term="otros"></category></entry><entry><title>Algoritmo de planificación Shortest Job First</title><link href="/hectormorago/alg-sjf.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/alg-sjf.html</id><summary type="html">&lt;p&gt;Algoritmo de planificación Shortest Job First.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;SJF&lt;/h2&gt;
&lt;p&gt;Primero el que tiene un tiempo de ráfaga más corto, SJF (Shortest Job First). Este algoritmo de planificación selecciona 
como proceso para ser ejecutado a aquel que tiene un &lt;strong&gt;tiempo de ráfaga de CPU más corto&lt;/strong&gt;. Es un algoritmo difícil de 
implementar en la práctica ya que requiere conocer por adelantado los tiempos de ráfaga de CPU de todos los procesos, lo cual
no es posible. En vez de utilizar el tiempo real de ráfaga de CPU de los distintos procesos, lo que se suele utilizar es una 
estimación del tiempo de ráfaga de CPU basado en los tiempos de ráfaga pasados. 
Existen dos implementaciones de este algoritmo en función de si se sigue una planificación de tipo cooperativa o expropiativa. 
Entre sus &lt;strong&gt;principales ventajas&lt;/strong&gt; se encuentran las siguientes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elimina algunos de los problemas del algoritmo FCFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sus principales desventajas son::&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difícil de implementar.&lt;/li&gt;
&lt;li&gt;Sobrecarga introducidad en el sistema debido al cálculo de las estimaciones del tiempo de ráfaga de los procesos.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category><category term="SSOO"></category><category term="algoritmos"></category><category term="otros"></category></entry><entry><title>Algoritmo de planificación Shortest Remaining Time</title><link href="/hectormorago/alg-srt.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/alg-srt.html</id><summary type="html">&lt;p&gt;Algoritmo de planificación Shortest Remaining Time.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;FCFS&lt;/h2&gt;
&lt;p&gt;Primero en llegar primero en ser servido, FCFS (First Come First Served). Es uno de los algoritmos de planificación 
más sencillos de implementar. El planificador maneja la cola de procesos como una cola FIFO. Entre sus ventajas destacamos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facilidad de implementación.&lt;/li&gt;
&lt;li&gt;Poca sobrecarga.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sus principales desventajas son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Penaliza los procesos con tiempos de ráfaga más cortos con respecto a aquellos que tienen tiempo de ráfaga más largos.&lt;/li&gt;
&lt;li&gt;Puede aparecer lo que se denomina como efecto convoy.&lt;/li&gt;
&lt;li&gt;Sufre la anomalía de belady.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SJF&lt;/h2&gt;
&lt;p&gt;Primero el que tiene un tiempo de ráfaga más corto, SJF (Shortest Job First). Este algoritmo de planificación selecciona 
como proceso para ser ejecutado a aquel que tiene un tiempo de ráfaga de CPU más pequeño. Este algoritmo es difícil de 
implementar en la práctica ya que requiere conocer por adelantado los tiempos de ráfaga de CPU de todos los procesos, lo cual
no es posible. Para su implementación lo que se suele utilizar es un estimación del tiempo de ráfaga de CPU basado en los 
tiempos de ráfaga pasados. Existen dos implementaciones de este algoritmo en función de si se sigue una planificación de tipo
cooperativa o expropiativa. Entre sus principales ventajas se encuentran las siguientes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elimina algunos de los problemas del algoritmo FCFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sus principales desventajas son::&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difícil de implementar.&lt;/li&gt;
&lt;li&gt;Sobrecarga introducidad en el sistema debido al calculo de las estimaciones del tiempo de ráfaga de los procesos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SRT&lt;/h2&gt;
&lt;h2&gt;Round Robin&lt;/h2&gt;</content><category term="blog"></category><category term="SSOO"></category><category term="algoritmos"></category><category term="otros"></category></entry><entry><title>Projects</title><link href="/hectormorago/projects.html" rel="alternate"></link><published>2020-04-06T20:31:00+02:00</published><updated>2020-04-06T20:31:00+02:00</updated><author><name>Hector Perez-Morago</name></author><id>tag:None,2020-04-06:/hectormorago/projects.html</id><summary type="html">&lt;p&gt;This section provides a summarize of my main projects.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Efficient Identification of Core and Dead Features in Variability Models&lt;/h2&gt;
&lt;h3&gt;Context&lt;/h3&gt;
&lt;p&gt;Variability models play a key role in product line engineering as they 
are used to represent the common and variable features that products may 
include, and what constraints among the features must be satisfied to 
guarantee the validity of the products. Valuable analysis operations on 
variability models can be performed by black box reusing logic engines, 
such as SAT-solvers and binary decision diagram libraries. Unfortunately, 
such kind of reuse implies long computation times for operations that 
need calling the engines many times. To overcome this problem, we work 
on new algorithms that directly deal with the data structure of a binary 
decision diagram encoding a variability model. In particular, the 
algorithm published in this site is specifically designed to detect core 
and dead features&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;
&lt;p&gt;This repository provides an algorithm to identify core and dead features, 
which is written in the C++ programming language as an extension of the 
&lt;a href="http://buddy.sourceforge.net/manual/main.html"&gt;BuDDy BDD package&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;More information about this project could be found in its&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://hperez30.github.io/CoreAndDeadFeatures/"&gt;main project page&lt;/a&gt;.&lt;/p&gt;</content><category term="Projects"></category><category term="research"></category><category term="projects"></category><category term="others"></category></entry></feed>